â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§¹ LIBERAR RAM PARA OLLAMA - m7i-flex.large
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SITUAÃ‡ÃƒO ATUAL:
  RAM Total: 7.6 GiB
  RAM Livre: 4.3 GiB
  RAM NecessÃ¡ria: 5.5 GiB
  FALTAM: 1.2 GiB

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ OPÃ‡ÃƒO 1: LIBERAR RAM (TENTAR FAZER OLLAMA FUNCIONAR)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Ver o que estÃ¡ usando RAM
docker stats --no-stream

# 2. Parar containers
cd ~/poc_multiagent_lang
docker compose down

# 3. Limpar cache do sistema
sudo sync
sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'

# 4. Ver RAM livre agora
free -h

# 5. Se tiver >5.5GB livre, subir containers
docker compose up -d

# 6. Aguardar Ollama carregar modelo (pode demorar 30s)
sleep 30

# 7. Verificar logs do Ollama
docker compose logs ollama | tail -20

# 8. Se ver "model loaded successfully", testar:
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ OPÃ‡ÃƒO 2: USAR MODELO MENOR (MAIS PROVÃVEL DE FUNCIONAR)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Em vez de llama2 (5.5GB RAM), use llama3.2:1b (1.3GB RAM):

# 1. Parar containers
docker compose down

# 2. Editar .env
nano .env

# Mudar de:
OLLAMA_MODEL=llama2

# Para:
OLLAMA_MODEL=llama3.2:1b

# Salvar: Ctrl+O, Enter, Ctrl+X

# 3. Subir containers
docker compose up -d

# 4. Aguardar e baixar modelo menor
sleep 30
docker compose exec ollama ollama pull llama3.2:1b

# 5. Testar
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ OPÃ‡ÃƒO 3: USAR GEMINI (MAIS RÃPIDO E CONFIÃVEL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Parar containers
docker compose down

# 2. Editar .env
nano .env

# Apagar:
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama2

# Adicionar:
LLM_TYPE=gemini
GEMINI_API_KEY=AIzaSyBVk3MFe3zRRGVMaEslphM3Vd85oS5Rz44
GEMINI_MODEL=gemini-2.0-flash

# Salvar: Ctrl+O, Enter, Ctrl+X

# 3. Subir SEM Ollama (economiza RAM)
docker compose up -d postgres backend frontend

# 4. Testar (serÃ¡ MUITO mais rÃ¡pido)
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š COMPARAÃ‡ÃƒO DE MODELOS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

llama2 (atual):
  RAM: 5.5 GB
  Qualidade: â­â­â­
  Velocidade: 60-120s
  Status: âŒ NÃ£o cabe na RAM disponÃ­vel

llama3.2:1b (alternativa):
  RAM: 1.3 GB
  Qualidade: â­â­
  Velocidade: 20-40s
  Status: âœ… Deve funcionar

gemini-2.0-flash (API):
  RAM: ~500 MB
  Qualidade: â­â­â­â­â­
  Velocidade: 2-5s
  Status: âœ… Funciona perfeitamente

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ RECOMENDAÃ‡ÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MELHOR OPÃ‡ÃƒO: Use Gemini API
  âœ… Qualidade superior
  âœ… 20x mais rÃ¡pido
  âœ… Sem problemas de RAM
  âœ… Sem problemas de setup

SEGUNDA OPÃ‡ÃƒO: Use llama3.2:1b
  âœ… Roda local (privacidade)
  âœ… Cabe na RAM disponÃ­vel
  âš ï¸  Qualidade inferior
  âš ï¸  Ainda lento

ÃšLTIMA OPÃ‡ÃƒO: Tentar liberar RAM para llama2
  âš ï¸  Pode nÃ£o funcionar
  âš ï¸  Muito lento mesmo se funcionar
  âš ï¸  InstÃ¡vel (pode crashar se RAM encher)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ DECISÃƒO RÃPIDA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Para POC/Testes: GEMINI
Para ProduÃ§Ã£o com dados sensÃ­veis: llama3.2:1b ou upgrade de instÃ¢ncia
