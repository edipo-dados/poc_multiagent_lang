â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ðŸ—‘ï¸  REMOVER OLLAMA - Liberar espaÃ§o                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Vamos remover o Ollama completamente e usar apenas APIs externas.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ§¹ PASSO 1: Remover Ollama do host
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Parar serviÃ§o
sudo systemctl stop ollama
sudo systemctl disable ollama

# Matar processos
pkill -9 ollama

# Remover binÃ¡rio
sudo rm -rf /usr/local/bin/ollama
sudo rm -rf /usr/bin/ollama

# Remover modelos e dados (~4GB)
rm -rf ~/.ollama

# Remover serviÃ§o systemd
sudo rm -f /etc/systemd/system/ollama.service
sudo rm -rf /etc/systemd/system/ollama.service.d
sudo systemctl daemon-reload

echo "âœ… Ollama removido do host"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ§¹ PASSO 2: Remover Ollama do Docker Compose
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

cd ~/poc_multiagent_lang

# Parar tudo
docker compose down

# Remover volume do Ollama
docker volume rm poc_multiagent_lang_ollama_data

# Atualizar docker-compose.yml (remover serviÃ§o ollama)
# Vou criar uma versÃ£o sem Ollama

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ”§ PASSO 3: Usar OpenAI (recomendado)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

cat > .env << 'EOF'
DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/regulatory_ai
LLM_TYPE=openai
OPENAI_API_KEY=sua-chave-openai
OPENAI_MODEL=gpt-3.5-turbo
EOF

# Obter chave: https://platform.openai.com/api-keys
# Tier gratuito: 3 RPM, 200 RPD
# Aguarde 1 minuto entre testes para evitar rate limit

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ’¾ ESPAÃ‡O LIBERADO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ollama no host: ~4GB (modelos)
Ollama no Docker: ~4GB (modelos + imagem)
Total liberado: ~8GB

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“Š COMPARAÃ‡ÃƒO: OpenAI vs Ollama
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OpenAI:
âœ… RÃ¡pido (2-5 segundos)
âœ… NÃ£o usa espaÃ§o em disco
âœ… NÃ£o usa CPU/RAM do servidor
âœ… Qualidade excelente
âŒ Rate limit (3 RPM no tier gratuito)
âŒ Precisa de internet

Ollama:
âœ… Sem rate limit
âœ… Funciona offline
âŒ Lento (60+ segundos)
âŒ Usa 4GB+ de disco
âŒ Usa CPU/RAM do servidor

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸš€ INICIAR SEM OLLAMA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Atualizar cÃ³digo
git pull origin main

# Iniciar apenas postgres, backend, frontend
docker compose up -d postgres backend frontend

# Aguardar
sleep 15

# Testar (aguarde 1 minuto se teve rate limit antes)
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
