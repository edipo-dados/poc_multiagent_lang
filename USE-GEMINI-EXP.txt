â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”§ USAR GEMINI-2.0-FLASH-EXP (MODELO EXPERIMENTAL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

O modelo gemini-1.5-flash estÃ¡ dando 404.
Vamos usar gemini-2.0-flash-exp (experimental, mais disponÃ­vel)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ COMANDOS PARA EXECUTAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Editar .env
nano .env

# 2. Mudar a linha:
GEMINI_MODEL=gemini-1.5-flash

# Para:
GEMINI_MODEL=gemini-2.0-flash-exp

# 3. Salvar: Ctrl+O, Enter, Ctrl+X

# 4. Parar e subir novamente
docker compose down
docker compose up -d postgres backend frontend

# 5. Aguardar 10 segundos
sleep 10

# 6. Testar
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š MODELOS GEMINI DISPONÃVEIS PARA TESTAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tente nesta ordem:

1. gemini-2.0-flash-exp (experimental, mais disponÃ­vel)
2. gemini-1.5-pro (mais estÃ¡vel, mas mais lento)
3. gemini-1.5-flash-latest (versÃ£o latest)
4. gemini-pro (versÃ£o antiga, mais compatÃ­vel)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” TESTAR MODELOS DISPONÃVEIS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Listar todos os modelos disponÃ­veis para sua API key
curl "https://generativelanguage.googleapis.com/v1beta/models?key=AIzaSyCL7u5UjG3NAduhLRszSRnd2hQHpIsHW74"

# Vai mostrar lista de modelos que vocÃª pode usar
# Escolha um que tenha "generateContent" nos supportedGenerationMethods

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸  SE NENHUM MODELO FUNCIONAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Use Ollama com modelo pequeno (llama3.2:1b):

# 1. Editar .env
nano .env

# 2. Mudar para:
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3.2:1b

# 3. Salvar e sair

# 4. Subir Ollama
docker compose up -d ollama

# 5. Aguardar e baixar modelo
sleep 30
docker compose exec ollama ollama pull llama3.2:1b

# 6. Parar e subir tudo
docker compose down
docker compose up -d

# 7. Testar
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

NOTA: llama3.2:1b usa apenas 1.3GB RAM e funciona offline
Mas Ã© mais lento (~20-40s) e qualidade inferior
