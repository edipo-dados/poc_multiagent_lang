â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âŒ PROBLEMA: RAM INSUFICIENTE PARA OLLAMA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ERRO IDENTIFICADO:
  model request too large for system
  requested="5.5 GiB" 
  available="4.3 GiB"
  total="7.6 GiB"

CAUSA:
  - InstÃ¢ncia t3.small tem 8GB RAM total
  - Sistema operacional usa ~3.7GB
  - Sobram apenas 4.3GB livres
  - llama2 precisa de 5.5GB para rodar
  - IMPOSSÃVEL rodar Ollama nesta instÃ¢ncia!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… SOLUÃ‡ÃƒO: USAR GEMINI API (ÃšNICA OPÃ‡ÃƒO VIÃVEL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Parar todos os containers
cd ~/poc_multiagent_lang
docker compose down

# 2. Editar .env
nano .env

# 3. Apagar estas linhas:
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama2

# 4. Adicionar estas linhas:
LLM_TYPE=gemini
GEMINI_API_KEY=AIzaSyBVk3MFe3zRRGVMaEslphM3Vd85oS5Rz44
GEMINI_MODEL=gemini-2.0-flash

# 5. Salvar e sair
# Ctrl+O, Enter, Ctrl+X

# 6. Subir APENAS os serviÃ§os necessÃ¡rios (SEM Ollama)
docker compose up -d postgres backend frontend

# 7. Verificar que estÃ¡ rodando
docker compose ps

# 8. Ver logs
docker compose logs -f backend

# 9. Testar API (deve funcionar em 2-5 segundos!)
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ LIBERAR ESPAÃ‡O (OPCIONAL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

JÃ¡ que nÃ£o vai usar Ollama, pode remover:

# 1. Remover volume do Ollama (~4GB)
docker volume rm poc_multiagent_lang_ollama_data

# 2. Remover Ollama do host (se instalado)
sudo systemctl stop ollama 2>/dev/null
sudo systemctl disable ollama 2>/dev/null
sudo rm -rf /usr/local/bin/ollama
sudo rm -rf ~/.ollama

# 3. Limpar imagens Docker nÃ£o usadas
docker image prune -a -f

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š COMPARAÃ‡ÃƒO: T3.SMALL vs REQUISITOS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

T3.SMALL (atual):
  RAM Total: 8GB
  RAM DisponÃ­vel: ~4.3GB
  âŒ Insuficiente para Ollama (precisa 5.5GB)
  âœ… Suficiente para Gemini API (usa ~500MB)

T3.MEDIUM (upgrade necessÃ¡rio para Ollama):
  RAM Total: 16GB
  RAM DisponÃ­vel: ~12GB
  âœ… Suficiente para Ollama
  ğŸ’° Custo: ~2x mais caro que t3.small

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ RECOMENDAÃ‡Ã•ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PARA POC/DESENVOLVIMENTO (t3.small):
  âœ… Use GEMINI API
  âœ… RÃ¡pido (2-5s por request)
  âœ… Sem problemas de RAM
  âœ… GrÃ¡tis com limites generosos
  âœ… Melhor qualidade de resposta

PARA PRODUÃ‡ÃƒO COM DADOS SENSÃVEIS:
  âœ… Upgrade para t3.medium ou maior
  âœ… Use Ollama local
  âœ… Privacidade total
  âŒ Mais lento (60s+ por request)
  âŒ Custo maior de infraestrutura

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ CONCLUSÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Com t3.small, Ollama Ã© IMPOSSÃVEL de rodar.

Suas opÃ§Ãµes:
1. âœ… Usar Gemini API (RECOMENDADO para POC)
2. ğŸ’° Fazer upgrade para t3.medium+ (se precisa Ollama)

Para este POC, Gemini Ã© a escolha certa:
  - Funciona na instÃ¢ncia atual
  - Muito mais rÃ¡pido
  - Sem custos adicionais de infraestrutura
  - Qualidade superior

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… RESULTADO ESPERADO COM GEMINI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Backend logs:
  âœ… "Initialized GeminiLLM with model=models/gemini-2.0-flash"
  âœ… Sentinel Agent completed in ~2-3 seconds
  âœ… All agents executing successfully

API response:
  âœ… Status 200 OK
  âœ… Complete analysis in 10-15 seconds total
  âœ… No timeouts or memory errors
