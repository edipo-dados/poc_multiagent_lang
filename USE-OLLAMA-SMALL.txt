â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”§ USAR OLLAMA COM MODELO PEQUENO (llama3.2:1b)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Gemini nÃ£o estÃ¡ funcionando (todos os modelos dÃ£o 404).
Vamos usar Ollama com modelo pequeno que cabe na RAM.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ COMANDOS PARA EXECUTAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Editar .env
nano .env

# 2. Substituir TODO o conteÃºdo por:
DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/regulatory_ai
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3.2:1b

# 3. Salvar: Ctrl+O, Enter, Ctrl+X

# 4. Parar tudo
docker compose down

# 5. Subir todos os serviÃ§os (incluindo Ollama)
docker compose up -d

# 6. Aguardar Ollama iniciar (30 segundos)
sleep 30

# 7. Baixar modelo pequeno llama3.2:1b (~1.3GB)
docker compose exec ollama ollama pull llama3.2:1b

# Aguardar download completar (2-5 minutos)

# 8. Verificar que modelo foi baixado
docker compose exec ollama ollama list

# Deve mostrar:
# NAME              ID              SIZE    MODIFIED
# llama3.2:1b       ...             1.3 GB  X seconds ago

# 9. Testar API
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š SOBRE O LLAMA3.2:1B
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Vantagens:
  âœ… Usa apenas 1.3GB RAM (cabe na instÃ¢ncia)
  âœ… Funciona offline (sem API keys)
  âœ… Sem custos
  âœ… Privacidade total

Desvantagens:
  âš ï¸  Mais lento (~20-40s por request)
  âš ï¸  Qualidade inferior ao Gemini
  âš ï¸  Respostas mais simples

Performance esperada:
  â±ï¸  Sentinel Agent: ~15-25 segundos
  â±ï¸  Translator Agent: ~20-30 segundos
  â±ï¸  Total: ~2-3 minutos

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… RESULTADO ESPERADO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Backend logs:
  âœ… "Initialized OllamaLLM with model=llama3.2:1b"
  âœ… "Sentinel Agent completed"
  âœ… "Translator Agent completed"

API response:
  âœ… Status 200 OK
  âœ… AnÃ¡lise completa (mais simples que Gemini)
  âœ… Tempo: ~2-3 minutos

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” MONITORAR PROGRESSO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Ver logs em tempo real
docker compose logs -f backend

# Ver uso de RAM
docker stats --no-stream

# Verificar que Ollama estÃ¡ rodando
docker compose ps

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ POR QUE GEMINI NÃƒO FUNCIONA?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Sua API key do Gemini nÃ£o tem acesso aos modelos.
PossÃ­veis causas:
  - Conta nova sem acesso aos modelos
  - RegiÃ£o bloqueada
  - Projeto sem Gemini API habilitada
  - RestriÃ§Ãµes da conta

Para usar Gemini, vocÃª precisaria:
  1. Criar projeto no Google Cloud Console
  2. Habilitar Generative Language API
  3. Criar API key no projeto
  4. Verificar que tem acesso aos modelos

Mas para POC, llama3.2:1b funciona bem!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ CONCLUSÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Para este POC, use llama3.2:1b:
  âœ… Funciona na instÃ¢ncia atual
  âœ… Sem problemas de API keys
  âœ… Sem custos
  âš ï¸  Mais lento mas funcional

Execute os comandos acima e aguarde o download do modelo!
