โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ  ๐ฏ SETUP FINAL - Ollama Docker sem IP fixo                  โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

SITUAรรO ATUAL:
โ Docker Compose configurado com Ollama
โ Ollama container rodando
โ DNS configurado (8.8.8.8, 8.8.4.4)
โ .env configurado (http://ollama:11434)
โ๏ธ  Modelo llama2 precisa ser baixado

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ PASSO A PASSO COMPLETO
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

# 1. Verificar status atual
bash check-status.sh

# 2. Se Ollama do host ainda estiver rodando, pare:
sudo systemctl stop ollama
sudo systemctl disable ollama

# 3. Reiniciar containers (se necessรกrio)
docker compose down
docker compose up -d

# 4. Aguardar serviรงos iniciarem
sleep 15

# 5. Baixar modelo llama2
bash download-llama2.sh

# 6. Testar API
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUรรO BCB Nยบ 789/2024","repo_path":"/app/fake_pix_repo"}'

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
COPIE TODO O BLOCO ACIMA
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ ARQUIVOS CRIADOS
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

โ download-llama2.sh       - Script para baixar modelo
โ check-status.sh          - Verificar status do sistema
โ DOWNLOAD-LLAMA2-NOW.txt  - Guia detalhado de download
โ FINAL-SETUP.txt          - Este arquivo

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ง ARQUIVOS MODIFICADOS
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

โ docker-compose.yml       - Adicionado serviรงo Ollama
โ .env                     - OLLAMA_BASE_URL=http://ollama:11434
โ backend/services/llm.py  - Suporte a Ollama, OpenAI, Gemini

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ฏ SOLUรรO IMPLEMENTADA
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

PROBLEMA: IP do host muda a cada reinicializaรงรฃo do servidor
SOLUรรO: Ollama como serviรงo Docker com nome fixo

ANTES:
โ OLLAMA_BASE_URL=http://172.31.5.241:11434  (IP muda!)

DEPOIS:
โ OLLAMA_BASE_URL=http://ollama:11434  (nome fixo!)

VANTAGENS:
โ Funciona apรณs reiniciar servidor
โ Sem dependรชncia de IP
โ Isolado em container
โ Dados persistentes (volume)
โ Health check automรกtico
โ Fรกcil de gerenciar

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ ARQUITETURA FINAL
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ  Docker Compose Network                                     โ
โ                                                              โ
โ  โโโโโโโโโโโโ    โโโโโโโโโโโโ    โโโโโโโโโโโโ             โ
โ  โ Frontend โโโโโถโ Backend  โโโโโถโ Ollama   โ             โ
โ  โ  :8501   โ    โ  :8000   โ    โ  :11434  โ             โ
โ  โโโโโโโโโโโโ    โโโโโโฌโโโโโโ    โโโโโโโโโโโโ             โ
โ                       โ                                     โ
โ                       โผ                                     โ
โ                  โโโโโโโโโโโโ                              โ
โ                  โ Postgres โ                              โ
โ                  โ  :5432   โ                              โ
โ                  โโโโโโโโโโโโ                              โ
โ                                                              โ
โ  Volumes:                                                   โ
โ  โข postgres_data  - Banco de dados                         โ
โ  โข ollama_data    - Modelos LLM                            โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

Todos os serviรงos se comunicam por NOME, nรฃo por IP!

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ๏ธ  CONFIGURAรรO ATUAL (.env)
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/regulatory_ai
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama2

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ COMANDOS รTEIS
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

# Ver status de tudo
bash check-status.sh

# Iniciar tudo
docker compose up -d

# Parar tudo
docker compose down

# Ver logs
docker compose logs -f backend
docker compose logs -f ollama

# Listar modelos Ollama
docker compose exec ollama ollama list

# Baixar outro modelo
docker compose exec ollama ollama pull mistral

# Reiniciar apenas um serviรงo
docker compose restart backend

# Ver uso de recursos
docker stats

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ TROUBLESHOOTING RรPIDO
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

Problema: "404 Not Found for url: http://ollama:11434"
Soluรงรฃo: Modelo nรฃo estรก baixado
โ bash download-llama2.sh

Problema: "address already in use" porta 11434
Soluรงรฃo: Ollama do host ainda rodando
โ sudo systemctl stop ollama
โ docker compose down && docker compose up -d

Problema: "connection refused" ao baixar modelo
Soluรงรฃo: DNS nรฃo estรก funcionando
โ docker compose down
โ docker compose up -d
โ sleep 15
โ bash download-llama2.sh

Problema: API retorna erro 500
Soluรงรฃo: Ver logs para detalhes
โ docker compose logs backend --tail 50

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ PERFORMANCE ESPERADA
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

Hardware: t3.small (2 vCPU, 2GB RAM, sem GPU)

Primeira requisiรงรฃo:
โข Carregamento do modelo: 10-30s
โข Processamento: 30-60s
โข Total: 60-90s

Requisiรงรตes seguintes:
โข Modelo jรก em memรณria
โข Processamento: 30-60s
โข Total: 30-60s

Isso รฉ NORMAL para CPU-only!

Para melhorar performance:
โข Upgrade para t3.medium (4GB RAM)
โข Usar instรขncia com GPU (g4dn.xlarge)
โข Ou usar API externa (OpenAI/Gemini)

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ SEGURANรA
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

โ๏ธ  IMPORTANTE: Vocรช expรดs sua chave OpenAI no chat!

REVOGUE IMEDIATAMENTE:
1. Acesse: https://platform.openai.com/api-keys
2. Revogue a chave: sk-proj-k1DSxrKn8UGV...
3. Crie uma nova chave
4. NUNCA compartilhe chaves em chat/cรณdigo pรบblico

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ CHECKLIST FINAL
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

Antes de testar, verifique:

โก Ollama do host estรก parado (systemctl stop ollama)
โก Containers estรฃo rodando (docker compose ps)
โก Modelo llama2 estรก baixado (ollama list)
โก .env tem OLLAMA_BASE_URL=http://ollama:11434
โก Backend estรก saudรกvel (curl localhost:8000)

Se todos โ, teste a API!

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐ PRรXIMO PASSO
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

Execute agora:

bash check-status.sh

Se modelo nรฃo estiver instalado:

bash download-llama2.sh

Depois teste:

curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUรรO BCB Nยบ 789/2024","repo_path":"/app/fake_pix_repo"}'

โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

