â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” DEBUG ERRO 500 DO OLLAMA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ERRO: 500 Server Error: Internal Server Error

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ COMANDOS PARA DIAGNOSTICAR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Ver logs do backend (Ãºltimas 50 linhas)
docker compose logs --tail=50 backend

# 2. Ver logs do Ollama
docker compose logs --tail=50 ollama

# 3. Verificar se Ollama estÃ¡ rodando
docker compose ps

# 4. Verificar se modelo llama2 estÃ¡ baixado
docker compose exec ollama ollama list

# 5. Testar Ollama diretamente
docker compose exec ollama ollama run llama2 "Hello"

# 6. Verificar conectividade do backend para Ollama
docker compose exec backend curl -s http://ollama:11434/api/tags

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”§ POSSÃVEIS CAUSAS E SOLUÃ‡Ã•ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CAUSA 1: Modelo llama2 nÃ£o foi baixado
SOLUÃ‡ÃƒO:
  docker compose exec ollama ollama pull llama2
  # Aguardar download (~4GB, pode demorar 5-10 min)

CAUSA 2: Ollama nÃ£o iniciou corretamente
SOLUÃ‡ÃƒO:
  docker compose restart ollama
  sleep 30
  docker compose exec ollama ollama list

CAUSA 3: Backend nÃ£o consegue conectar no Ollama
SOLUÃ‡ÃƒO:
  # Verificar se estÃ£o na mesma rede Docker
  docker network inspect poc_multiagent_lang_default
  
  # Testar conectividade
  docker compose exec backend ping -c 3 ollama

CAUSA 4: Timeout na requisiÃ§Ã£o (Ollama muito lento)
SOLUÃ‡ÃƒO:
  # Aumentar timeout no cÃ³digo ou usar modelo menor
  # Ou trocar para Gemini API (mais rÃ¡pido)

CAUSA 5: MemÃ³ria insuficiente
SOLUÃ‡ÃƒO:
  # Verificar uso de memÃ³ria
  docker stats --no-stream
  
  # Se necessÃ¡rio, usar Gemini em vez de Ollama

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ SOLUÃ‡ÃƒO RÃPIDA: TROCAR PARA GEMINI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Se Ollama continuar com problemas, use Gemini (mais rÃ¡pido e confiÃ¡vel):

# 1. Parar containers
docker compose down

# 2. Editar .env
nano .env

# Mudar de:
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama2

# Para:
LLM_TYPE=gemini
GEMINI_API_KEY=AIzaSyBVk3MFe3zRRGVMaEslphM3Vd85oS5Rz44
GEMINI_MODEL=gemini-2.0-flash

# 3. Subir sem Ollama (economiza recursos)
docker compose up -d postgres backend frontend

# 4. Testar
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š COMPARAÃ‡ÃƒO: OLLAMA vs GEMINI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OLLAMA (Local):
  âœ… GrÃ¡tis
  âœ… Privacidade total
  âŒ Lento (60+ segundos por request)
  âŒ Usa ~4GB de disco
  âŒ Usa muita RAM (~2-4GB)
  âŒ Requer download de modelo

GEMINI (API):
  âœ… RÃ¡pido (2-5 segundos por request)
  âœ… NÃ£o usa recursos locais
  âœ… Modelos mais avanÃ§ados
  âœ… Sem setup necessÃ¡rio
  âŒ Requer API key
  âŒ Tem limites de rate (mas generosos)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¡ RECOMENDAÃ‡ÃƒO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Para POC/desenvolvimento: Use GEMINI
  - Mais rÃ¡pido
  - Menos problemas
  - Economiza recursos do EC2

Para produÃ§Ã£o com dados sensÃ­veis: Use OLLAMA
  - Dados nÃ£o saem do servidor
  - Sem custos de API
  - Mas precisa de servidor maior (t3.medium ou maior)
