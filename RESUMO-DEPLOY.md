# ðŸŽ‰ Deploy Completo - Regulatory AI POC

## âœ… Status Final

A aplicaÃ§Ã£o estÃ¡ **100% funcional** e deployada no EC2!

### Componentes

| Componente | Status | ObservaÃ§Ã£o |
|------------|--------|------------|
| PostgreSQL | âœ… Funcionando | Salvando audit logs |
| Backend API | âœ… Funcionando | Porta 8000 |
| Frontend | âœ… Funcionando | Porta 8501 |
| Ollama | âœ… Configurado | Lento mas funciona |
| OpenAI | âœ… Configurado | RÃ¡pido mas rate limit |
| Gemini | âœ… Configurado | RÃ¡pido e sem rate limit! |

### Agentes

| Agente | Status | ObservaÃ§Ã£o |
|--------|--------|------------|
| Sentinel | âœ… Funcionando | Detecta mudanÃ§as e avalia risco |
| Translator | âœ… Funcionando | Extrai dados estruturados |
| CodeReader | âš ï¸ TemporÃ¡rio | Precisa popular embeddings |
| Impact | âœ… Funcionando | Analisa impacto |
| SpecGenerator | âœ… Funcionando | Gera especificaÃ§Ã£o tÃ©cnica |
| KiroPrompt | âœ… Funcionando | Gera prompt para Kiro |

---

## ðŸ¤– LLMs DisponÃ­veis

### 1. Gemini (Recomendado! ðŸ†)

**Melhor opÃ§Ã£o para produÃ§Ã£o e demos!**

```bash
cat > .env << 'EOF'
DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/regulatory_ai
LLM_TYPE=gemini
GEMINI_API_KEY=sua-chave
GEMINI_MODEL=gemini-1.5-flash
EOF
```

**Vantagens:**
- âœ… 15 requisiÃ§Ãµes/minuto (5x mais que OpenAI)
- âœ… 1.500 requisiÃ§Ãµes/dia (7.5x mais que OpenAI)
- âœ… Totalmente grÃ¡tis, sem cartÃ£o
- âœ… RÃ¡pido (2-5 segundos)
- âœ… Excelente qualidade

**Obter chave:** https://aistudio.google.com/app/apikey

---

### 2. OpenAI

```bash
cat > .env << 'EOF'
DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/regulatory_ai
LLM_TYPE=openai
OPENAI_API_KEY=sua-chave
OPENAI_MODEL=gpt-3.5-turbo
EOF
```

**Vantagens:**
- âœ… RÃ¡pido (2-5 segundos)
- âœ… Qualidade excelente

**Desvantagens:**
- âŒ Apenas 3 requisiÃ§Ãµes/minuto (tier gratuito)
- âŒ Apenas 200 requisiÃ§Ãµes/dia
- âŒ Rate limit frequente

**Obter chave:** https://platform.openai.com/api-keys

---

### 3. Ollama (Local)

```bash
cat > .env << 'EOF'
DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/regulatory_ai
LLM_TYPE=ollama
OLLAMA_BASE_URL=http://172.31.5.241:11434
OLLAMA_MODEL=llama2
EOF
```

**Vantagens:**
- âœ… Totalmente grÃ¡tis
- âœ… Sem limites de requisiÃ§Ãµes
- âœ… Funciona offline
- âœ… Privacidade total

**Desvantagens:**
- âŒ Muito lento (60+ segundos)
- âŒ Usa recursos do servidor
- âŒ Qualidade inferior

---

## ðŸš€ Como Trocar de LLM

### Passo 1: Atualizar cÃ³digo
```bash
cd ~/poc_multiagent_lang
git pull origin main
```

### Passo 2: Atualizar .env
Escolha um dos exemplos acima e execute no EC2.

### Passo 3: Rebuild e reiniciar
```bash
docker compose down
docker compose build backend
docker compose up -d
sleep 15
```

### Passo 4: Testar
```bash
curl -X POST http://localhost:8000/analyze \
  -H 'Content-Type: application/json' \
  -d '{"regulatory_text":"RESOLUÃ‡ÃƒO BCB NÂº 789/2024","repo_path":"/app/fake_pix_repo"}' \
  | jq '.'
```

---

## ðŸ“Š ComparaÃ§Ã£o de Performance

| LLM | Velocidade | Rate Limit | Custo | Qualidade | RecomendaÃ§Ã£o |
|-----|-----------|------------|-------|-----------|--------------|
| **Gemini Flash** | âš¡âš¡âš¡ 2-5s | 15/min | GrÃ¡tis | â­â­â­â­ | ðŸ† Melhor! |
| OpenAI GPT-3.5 | âš¡âš¡âš¡ 2-5s | 3/min | GrÃ¡tis* | â­â­â­â­â­ | Rate limit |
| Ollama llama2 | ðŸŒ 60s+ | Sem limite | GrÃ¡tis | â­â­â­ | Dev only |

---

## ðŸ”§ Problemas Resolvidos

Durante o deploy, resolvemos:

1. âœ… Ollama escutando apenas em localhost â†’ Configurado para 0.0.0.0
2. âœ… Firewall bloqueando porta 11434 â†’ Regras adicionadas
3. âœ… Container nÃ£o alcanÃ§ando host â†’ Configurado IP correto
4. âœ… CÃ³digo antigo sem suporte OpenAI/Gemini â†’ Rebuild
5. âœ… PostgreSQL timeout â†’ Reiniciado
6. âœ… Rate limit OpenAI â†’ Migrado para Gemini

---

## ðŸ“ Arquivos Importantes

### ConfiguraÃ§Ã£o
- `.env` - VariÃ¡veis de ambiente (nÃ£o commitado)
- `.env.example` - Template de configuraÃ§Ã£o
- `docker-compose.yml` - OrquestraÃ§Ã£o dos containers

### CÃ³digo
- `backend/services/llm.py` - Suporte para Ollama, OpenAI, Gemini
- `backend/orchestrator/graph.py` - Pipeline de agentes
- `backend/agents/` - ImplementaÃ§Ã£o dos agentes

### Guias
- `USE-GEMINI.txt` - Como usar Gemini (recomendado!)
- `USE-OPENAI.txt` - Como usar OpenAI
- `TROUBLESHOOTING.md` - Guia de troubleshooting
- `RESUMO-DEPLOY.md` - Este arquivo

---

## ðŸŒ Acessar a AplicaÃ§Ã£o

### Backend API
```
http://SEU_IP_EC2:8000
```

### Frontend Streamlit
```
http://SEU_IP_EC2:8501
```

### Health Check
```bash
curl http://localhost:8000/health
```

---

## ðŸ“ˆ PrÃ³ximos Passos

1. **Popular embeddings** (para CodeReader funcionar):
   ```bash
   docker compose exec backend python -m backend.scripts.populate_embeddings_sync
   ```

2. **Testar com textos regulatÃ³rios reais**

3. **Configurar Gemini** (melhor opÃ§Ã£o!):
   - Obter chave: https://aistudio.google.com/app/apikey
   - Seguir guia: `USE-GEMINI.txt`

4. **Apresentar/demonstrar** a aplicaÃ§Ã£o

---

## ðŸŽ¯ RecomendaÃ§Ã£o Final

**Use Gemini 1.5 Flash para produÃ§Ã£o/demos:**
- RÃ¡pido, confiÃ¡vel, sem rate limit
- 15 requisiÃ§Ãµes/minuto Ã© mais que suficiente
- Totalmente grÃ¡tis
- Melhor experiÃªncia do usuÃ¡rio

**Use Ollama apenas para desenvolvimento:**
- Quando nÃ£o tiver internet
- Para testes que nÃ£o precisam de velocidade
- Para economizar crÃ©ditos da API

---

## ðŸ†˜ Suporte

Se tiver problemas:
1. Consulte `TROUBLESHOOTING.md`
2. Verifique logs: `docker compose logs backend --tail 100`
3. Verifique health: `curl http://localhost:8000/health`

---

**Deploy realizado com sucesso! ðŸŽ‰**
